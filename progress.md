* [ ] **LeNet-5** (1998) – *Gradient-Based Learning Applied to Document Recognition*
* **Focus:** Convolutional layers, Subsampling, and the birth of CNNs.
* [ ] **AlexNet** (2012) – *ImageNet Classification with Deep CNNs*
* **Focus:** ReLU, Dropout, and GPU-accelerated training.
* [ ] **VGG** (2014) – *Very Deep Convolutional Networks for Large-Scale Image Recognition*
* **Focus:** Uniform  filters and standardizing deep architectures.
* [ ] **Network in Network (NiN)** (2013) – *Network In Network*
* **Focus:**  Convolutions and Global Average Pooling.
* [ ] **GoogLeNet / Inception** (2015) – *Going Deeper with Convolutions*
* **Focus:** Inception modules and multi-scale feature extraction.
* [ ] **ResNet** (2016) – *Deep Residual Learning for Image Recognition*
* **Focus:** Skip connections and the identity mapping.
* [ ] **ResNeXt** (2017) – *Aggregated Residual Transformations for Deep Neural Networks*
* **Focus:** Grouped convolutions and "Cardinality."
* [ ] **DenseNet** (2017) – *Densely Connected Convolutional Networks*
* **Focus:** Feature reuse and the Growth Rate.
* [ ] **R-CNN Family** (2014+) – *Rich feature hierarchies for accurate object detection*
* **Focus:** Region proposals and bounding box regression logic.
* [ ] **SSD (Single Shot Multibox Detector)** (2016) – *SSD: Single Shot MultiBox Detector*
* **Focus:** Multi-scale feature maps for detection.
* [ ] **FCN (Fully Convolutional Networks)** (2015) – *Fully Convolutional Networks for Semantic Segmentation*
* **Focus:** Upsampling, Transposed Convolutions, and pixel-wise prediction.

* [ ] **word2vec** (2013) – *Efficient Estimation of Word Representations in Vector Space*
* **Focus:** Skip-gram and Negative Sampling from scratch.
* [ ] **GloVe** (2014) – *GloVe: Global Vectors for Word Representation*
* **Focus:** Log-bilinear regression on global co-occurrence statistics.
* [ ] **BERT** (2018) – *Pre-training of Deep Bidirectional Transformers*
* **Focus:** Masked LM (MLM) and Next Sentence Prediction (NSP).

* [ ] **The Transformer** (2017) – *Attention Is All You Need*
* **Focus:** Multi-Head Self-Attention and Positional Encoding.
* [ ] **Vision Transformer (ViT)** (2020) – *An Image is Worth 16x16 Words*
* **Focus:** Patch embeddings and applying Transformers to vision.

* [ ] **LSTM** (1997) – *Long Short-Term Memory*
* **Focus:** The "Constant Error Carousel" and gating mechanisms.
* [ ] **GRU** (2014) – *Learning Phrase Representations using RNN Encoder-Decoder*
* **Focus:** Simplified gates (Reset/Update) and sequence modeling.

* [ ] **GAN** (2014) – *Generative Adversarial Nets*
* **Focus:** The Minimax game and adversarial training stability.
* [ ] **DCGAN** (2015) – *Unsupervised Representation Learning with DCGANs*
* **Focus:** Strided convolutions and Batch Norm in generative tasks.
